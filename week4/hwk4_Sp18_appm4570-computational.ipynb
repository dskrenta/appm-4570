{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #4: Computational Questions\n",
    "*All computations should be done in this notebook using the R kernel. Working in small groups is allowed, but it is important that you make an effort to master the material and hand in your own work.*\n",
    "\n",
    "#### You will be required to submit this notebook, fully compiled with your solutions, as an HTML or ipynb file to Canvas by 2pm on Friday, Feburary 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $X =$ the leading digit of a randomly selected number from a large accounting ledger. So, for example, if we randomly draw the number $\\$20,695$, then $X = 2$. People who make up numbers to commit accounting fraud tend to give $X$ a (discrete) uniform distribution, i.e., $P(X = x) = 1/9$, for $x \\in \\{1,...,9\\}$. However, there is empirical evidence that suggests that ``naturally occurring\" numbers (e.g., numbers in a non-fraudulent accounting ledger) have leading digits that do not follow a uniform distribution. Instead, they follow a distribution defined by:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) = \\log_{10}\\bigg(\\frac{x+1}{x}\\bigg), \\,\\,\\,\\, x = 1,2,...,9.\n",
    "\\end{align*} \n",
    "\n",
    "#### Using ${\\tt tax.txt}$, a dataset containing the taxable incomes for individuals in 1978, and the information given in theoretical problem 5, decide whether this dataset is fraudulent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Consider the scenario from Unit #2 in-class code:\n",
    "\n",
    "Imagine two unmarked bags filled with poker chips. Each bag contains both red poker chips and white poker chips. In bag one, there are 75% red and 25% white; in bag two, there are 75% white and 25% red. Imagine that you pick one bag at random, and, without looking inside, begin to draw chips out, one at a time, replacing and mixing after each draw, so that each draw is independent.\n",
    "\n",
    "\n",
    "#### (a) Write a function called ${\\tt draws}$, with one argument $n$, that randomly selects one of the two bags, and then generates a sample of size $n = 20$ from it (i.e., draws $n = 20$ chips, with replacement). Let 0 = \"White\" and 1 = \"Red\". Then, execute the line ${\\tt e = draws(n)}$. The variable ${\\tt e}$ will consist of a string of 1s and 0s, representing what happened as you drew chips. (HINT: The functions ${\\tt rbinom()}$ and [${\\tt switch()}$](https://www.datamentor.io/r-programming/switch-function) might be useful in randomly choosing the bag.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal for the rest of the question is to use Bayes' theorem to calculuate our degree of belief in the hypothesis $H$ = \"We've selected bag one\" given the evidence $e_i$ = \"we've drawn $x$ red chips in $i$ draws\", $i = 1,...,n$. Remember, Bayes' theorem is\n",
    "\n",
    "$$P(H \\, | \\, e_i) = \\frac{P(e_i \\, | \\, H)P_i(H)}{P(e_i \\, | \\, H)P_i(H) + P(e_i \\, | \\, H^c)P_i(H^c)}.$$\n",
    "\n",
    "Achieving our goal will involve setting $P_i(H)$--the prior probability for $H$ at step $i$--equal to $P(H \\, | \\, e_{i-1})$--the posterior probability for $H$ from step $i-1$.\n",
    "\n",
    "#### (b) Constuct an $n \\times 2$ matrix $p$ where the $i^{th}$ entry of the first column contains $P(e_i \\, | \\, H)$ and the $i^{th}$ entry of the second column contains $P(e_i \\, | \\, H^c)$.  Note: These are binomial probabilities, and can be calculated--without a loop!--using ${\\tt dbinom()}$ and  ${\\tt cumsum()}$.  ${\\tt cbind()}$ can help form that matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Create an $n \\times 2$ matrix ${\\tt prior}$ that has 0.5 in the (1,1) and (1,2) entry, and NA everywhere else (for now). This is where we'll store $P_i(H)$ and $P_i(H^c)$ for every step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Create an $n \\times 1$ vector ${\\tt post}$  with NAs everywhere. This is where we'll store $P(H \\, | \\, e_i)$, the posterior probability for $H$ at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Use the function below to calculate the posterior probability for the first step, i.e., after pulling one chip. Store this in ${\\tt post[1]}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = function(p, prior){\n",
    "    #p is a vector of size 2, with likelihoods for each bag\n",
    "    #prior is a vector of size 2, with priors for each bag\n",
    "    post = p[1]*prior[1]/(p[1]*prior[1] + p[2]*prior[2])\n",
    "    return(post)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) In the loop below, add two lines of code. The first line should use the posterior probability from the $(i-1)$ step to store $P_i(H)$ and $P_i(H^c)$ in ${\\tt prior}$ for the $i^{th}$ step. The second line should calculate the posterior probability for the $i^{th}$ step (using the ${\\tt posterior}$ function above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'n' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'n' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "for (i in 2:n){\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g) Use ${\\tt data.frame()}$ to construct a table containing, at each step $i = 1,...,n$,  the posterior distribution, $P(H \\, | \\, e_i)$, the likelihood $P(e_i \\, | \\, H)$, and the prior $P_i(H)$. Then, construct a plot of the posterior probability against $i$, for $i = 1,...,n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (h) After $n = 25$ draws, what do you believe about $H$? Based on your table and plot, how many chips did you need to draw before you became basically certain about the truth or falsity of $H$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
